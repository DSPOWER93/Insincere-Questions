{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bi-LSTM-Quora insincere question Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP0p6Zx6/5E5ts+9FmCFih/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DSPOWER93/quora-insincere/blob/main/Bi_LSTM_insincere_question_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQfHQwP0wPDO"
      },
      "source": [
        "### Synopsis:\n",
        "\n",
        "The Following file is a working on NLP classifier to identify insincere questions. Data is taken from kaggle Competition [Quora Insicere Question Classification](https://www.kaggle.com/c/quora-insincere-questions-classification).\n",
        "\n",
        "- **Champion Model**: Bi-Directional LSTM + Conv1D\n",
        "- **Params**:\n",
        "- **Framework**:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApNXoq8RE7S2"
      },
      "source": [
        "#### Mounting G-drive to get training & embedding Data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftIpBTgswl_P",
        "outputId": "7591fb32-5842-4782-9aa5-1b4595505c55"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SngfZe6CwdFj"
      },
      "source": [
        "Importing python Library & packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sEjmHEwTDzL"
      },
      "source": [
        "#  Importing base Libraries.\n",
        "import numpy as np\n",
        "np.set_printoptions(suppress=True)\n",
        "import pandas as pd\n",
        "import re, os \n",
        "\n",
        "\n",
        "# TensorFlow & Keras Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import initializers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D, Dropout, CuDNNLSTM\n",
        "from keras.initializers import Constant\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "# For Parallel Processing\n",
        "import multiprocessing\n",
        "import concurrent.futures as c"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Remaining packages"
      ],
      "metadata": {
        "id": "gT2-b3Rh9O7X"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXpHwc1DfwN3"
      },
      "source": [
        "%%capture\n",
        "!pip install pyspellchecker"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2_XHZ2sfaFD"
      },
      "source": [
        "### Importing the raw Data\n",
        "Quora's question classfier data is stored on gdrive for easy loading."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_iLQtb4fcij"
      },
      "source": [
        "# Loading csv file\n",
        "# Location to be changed as per file location\n",
        "df = pd.read_csv('/content/drive/MyDrive/Quora_project/train.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mmk2Q6wLfi9b",
        "outputId": "7b122868-5c47-40a1-f896-c960b9c25925"
      },
      "source": [
        "#  Instpecting the Dataset\n",
        "print(df.info())\n",
        "\n",
        "print(df.groupby('target').count()/ len(df) * 100)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1306122 entries, 0 to 1306121\n",
            "Data columns (total 3 columns):\n",
            " #   Column         Non-Null Count    Dtype \n",
            "---  ------         --------------    ----- \n",
            " 0   qid            1306122 non-null  object\n",
            " 1   question_text  1306122 non-null  object\n",
            " 2   target         1306122 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 29.9+ MB\n",
            "None\n",
            "              qid  question_text\n",
            "target                          \n",
            "0       93.812982      93.812982\n",
            "1        6.187018       6.187018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing pre-processing libraries"
      ],
      "metadata": {
        "id": "GWhv6ckZ_3Xb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVlrajnXf1Wz"
      },
      "source": [
        "# Importing Libraries for NLP \n",
        "import re\n",
        "import spacy\n",
        "spacy.prefer_gpu()\n",
        "import string\n",
        "\n",
        "# to make spacy work in pipeline mode.\n",
        "nlp_vocab = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'ner'])\n",
        "nlp_vocab.add_pipe(nlp_vocab.create_pipe('sentencizer'))\n",
        "\n",
        "# Importing spellchecker & NLP\n",
        "from spellchecker import SpellChecker"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j0_uU-LMsAd"
      },
      "source": [
        "### Corpus Cleaning\n",
        "\n",
        "Defining function to clean corpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G_zKbrwqz3w"
      },
      "source": [
        "# lemmatization of words from spacy. \n",
        "def spacy_lemmatize(x):\n",
        "  x = nlp_vocab(x)\n",
        "  x = [s.lemma_ for s in x]\n",
        "  x = \" \".join(x)\n",
        "  return x\n",
        "\n",
        "# Spelling collection \n",
        "spell = SpellChecker()\n",
        "def correct_spellings(x, spell=spell):\n",
        "    \"\"\"correct the misspelled words of a given corpus\"\"\"\n",
        "    x = x.split()\n",
        "    misspelled = spell.unknown(x)\n",
        "    result = map(lambda word : spell.correction(word) if word in  misspelled else word, x)\n",
        "    return \" \".join(result)\n",
        "\n",
        "# corpus cleaning. keeping Lemmatization default as False for pre-processing as it's time serialbased activity we will later use it in parallel computing.\n",
        "def corpus_cleaning(x, correct_spelling=True, remove_emojis=True, remove_stop_words=False, lemmatize=False):\n",
        "    \"\"\"Apply function to a clean a corpus\"\"\"\n",
        "    x = x.lower().strip()\n",
        "    # romove urls\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    x = url.sub(r'',x)\n",
        "    # remove html tags\n",
        "    html = re.compile(r'<.*?>')\n",
        "    x = html.sub(r'',x)\n",
        "    # remove punctuation\n",
        "    operator = str.maketrans('','',string.punctuation) #????\n",
        "    x = x.translate(operator)\n",
        "    if correct_spelling:\n",
        "        x = correct_spellings(x)\n",
        "    if lemmatize:\n",
        "        x = spacy_lemmatize(x)\n",
        "    if remove_emojis:\n",
        "        x = x.encode('ascii', 'ignore').decode('utf8').strip()\n",
        "    if remove_stop_words:\n",
        "        x = ' '.join([word for word in x.split(' ') if word not in stop_words])\n",
        "    return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhnH4ZmgBL8Y"
      },
      "source": [
        "### Lemmatization using parallel processing\n",
        "\n",
        "Spacy in general creates metadata of each corpus element which is quite of time consuming task in general. This creates room for parallel processing as running for loop will be doing sequential job not utilizing potential power of computing instance. The codes used to execute were used from well witten article on Spacy for parallel processing. [Link](https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9UirzGhLGiN"
      },
      "source": [
        "#%%time\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "def lemmatize_pipe(doc):\n",
        "    lemma_list = [s.lemma_ for s in doc] \n",
        "    return lemma_list\n",
        "    \n",
        "def preprocess_pipe(texts):\n",
        "    preproc_pipe = []\n",
        "    for doc in nlp_vocab.pipe(texts, batch_size=20):\n",
        "        preproc_pipe.append(lemmatize_pipe(doc))\n",
        "    return preproc_pipe\n",
        "\n",
        "def chunker(iterable, total_length, chunksize):\n",
        "    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n",
        "\n",
        "def flatten(list_of_lists):\n",
        "    \"Flatten a list of lists to a combined list\"\n",
        "    return [item for sublist in list_of_lists for item in sublist]\n",
        "\n",
        "def process_chunk(texts):\n",
        "    preproc_pipe = []\n",
        "    for doc in nlp_vocab.pipe(texts, batch_size=20):\n",
        "        preproc_pipe.append(lemmatize_pipe(doc))\n",
        "    return preproc_pipe\n",
        "\n",
        "def preprocess_parallel(texts, chunksize=1000):\n",
        "    executor = Parallel(n_jobs=7, backend='multiprocessing', prefer=\"processes\")\n",
        "    do = delayed(process_chunk)\n",
        "    tasks = (do(chunk) for chunk in chunker(texts, len(texts), chunksize=chunksize))\n",
        "    result = executor(tasks)\n",
        "    return flatten(result)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sample test code for Cleaning using Parallelization."
      ],
      "metadata": {
        "id": "26Ow3d9-BuqR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OZiNTI6aU0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2849c6f7-6cb0-4815-f1ab-407fa81c63c1"
      },
      "source": [
        "import multiprocessing\n",
        "pool = multiprocessing.Pool()\n",
        "\n",
        "corpuss = ['I am not, sure whether  halth living this is an rigth answer', 'greatness', 'are you even seriuos', 'please dont tell its a joke']\n",
        "\n",
        "preprocess_parallel(corpuss)\n",
        "pool_processing = pool.map(corpus_cleaning, corpuss)\n",
        "print(pool_processing)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i am not sure whether health living this is an rigth answer', 'greatness', 'are you even serious', 'please dont tell its a joke']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQx6fb4I7ylD"
      },
      "source": [
        "### View of original Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryYlbRf37yDk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bfcfeab-28cf-4b11-c988-143dd94b2cc9"
      },
      "source": [
        "# normal questions\n",
        "print(df[df['target'] == 0]['question_text'][:5])\n",
        "# immature questions\n",
        "print(df[df['target'] == 1]['question_text'][:5])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    How did Quebec nationalists see their province...\n",
            "1    Do you have an adopted dog, how would you enco...\n",
            "2    Why does velocity affect time? Does velocity a...\n",
            "3    How did Otto von Guericke used the Magdeburg h...\n",
            "4    Can I convert montra helicon D to a mountain b...\n",
            "Name: question_text, dtype: object\n",
            "22     Has the United States become the largest dicta...\n",
            "30     Which babies are more sweeter to their parents...\n",
            "110    If blacks support school choice and mandatory ...\n",
            "114    I am gay boy and I love my cousin (boy). He is...\n",
            "115                 Which races have the smallest penis?\n",
            "Name: question_text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWtWyOQbUxj0"
      },
      "source": [
        "### Cleaning of Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reWKAPPIFnTv"
      },
      "source": [
        "Before moving to corpus Cleaning demonstrating advantage and comparison of using parallel processing in pre-processing using small textual corpus, comparing traditional sequential Methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-YCBFEQ2rpN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2542d18-441a-454e-991c-c6bec441b9e6"
      },
      "source": [
        "%%time\n",
        "dd = list(df[:500]['question_text'])\n",
        "fd = [corpus_cleaning(k) for k in dd]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 33.4 s, sys: 112 ms, total: 33.5 s\n",
            "Wall time: 33.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUR-QokOBOHx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e0c5b72-9c23-447d-e27c-1e8a681a2ac8"
      },
      "source": [
        "%%time\n",
        "with c.ProcessPoolExecutor() as executor:\n",
        "  results = [ k for k in executor.map(corpus_cleaning,dd)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 224 ms, sys: 107 ms, total: 331 ms\n",
            "Wall time: 12.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwA4q0HtF6Na",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8950faea-b782-4fcb-8f39-d1f23bc1a764"
      },
      "source": [
        "fd == results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSZJtizZQonl"
      },
      "source": [
        "#### Observation\n",
        "\n",
        "The Computational time has reduced by almost more than 50% for Calculation. we would inculcate the option "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo3vVzwNhIZi"
      },
      "source": [
        "#### One time activity of cleaning the Data\n",
        "\n",
        "\n",
        "We will Proceed ahead with cleaning the data as one time activity of one million records. As cleaning Large Data takes quite amount of time. The objective is that not to repeat the cleaning process again in analysis. We will split the Data in small Parts and download seperately, Because if Python crashes midway we can resume from checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLlyw3pnUxHm"
      },
      "source": [
        "'''\n",
        "# %%time\n",
        "# corpus transformation.\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "corpus = list(df[0:1000000]['question_text'])\n",
        "target =  list(df[0:1000000]['target'])\n",
        "rows = list(range(0,len(corpus)))\n",
        "\n",
        "f = 0 \n",
        "for i in range(0,len(corpus), int(len(corpus)/5)):\n",
        "  f =  f+1\n",
        "  iter = corpus[i:i+int(len(corpus)/5)]\n",
        "  iter_target = target[i:i+int(len(corpus)/5)]\n",
        "  iter_rows = rows[i:i+int(len(corpus)/5)]\n",
        "  print('completion {}'.format(i),'/{}'.format(len(corpus)))\n",
        "  # using parallel processing to complete to clean the corpus\n",
        "  with c.ProcessPoolExecutor() as executor:\n",
        "    my_list = [ k for k in executor.map(corpus_cleaning,iter)]\n",
        "  # Parallel computing of lemmatization. \n",
        "  my_list = preprocess_parallel(my_list)\n",
        "  joined_corpus =[]\n",
        "  for l in my_list:\n",
        "    joined_corpus.append(\" \".join(l))\n",
        "  iter_df = pd.DataFrame({\n",
        "      'Rows' : iter_rows,\n",
        "      'Text' : joined_corpus,\n",
        "      'target': iter_target\n",
        "  })\n",
        "  iter_df.to_csv('clean_data_'+str(f)+'.csv')\n",
        "  files.download('clean_data_'+str(f)+'.csv') \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fCl_cWaQj7L"
      },
      "source": [
        "### Importing Cleaned Data from above step\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi8yT16rQjn7"
      },
      "source": [
        "clean_df = pd.read_csv('/content/drive/MyDrive/Quora_project/final_data.csv', index_col= 'Unnamed: 0').reset_index()\n",
        "\n",
        "# Drop Null entries\n",
        "clean_df = clean_df[clean_df['Text'].notnull()]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rtslIqwSigA"
      },
      "source": [
        "### Stop Words Removal \n",
        "#### (In order text to maintain the sentiment context better haven't dropped stop words in analysis)\n",
        "\n",
        "Removing specific Stop Words from Data set which wouldn't be having much impact on the corpus Data. Usually the Stop Words from existing Libraries would be having including Negated words like No, Not, aren't, can't etc... which does have impact on sentiment present in text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7p_MX44S2JS"
      },
      "source": [
        "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \n",
        "             \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\",\n",
        "             \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \n",
        "             \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\",\n",
        "             \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\",\n",
        "             \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \n",
        "             \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\",\n",
        "             \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\",\n",
        "             \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\",\n",
        "             \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\",\n",
        "             \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "\n",
        "def drop_stopwords(x): \n",
        "    corpus = \" \".join([word for word in x.split() if word not in (stopwords)])\n",
        "    return corpus\n",
        "drop_stopwords('i am happy who is this')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEMHwk_7Ve1E"
      },
      "source": [
        "# cleaning stop words\n",
        "# clean_df['Text'] =  clean_df['Text'].apply(drop_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCFNVZMSZPPN"
      },
      "source": [
        "#### Inspecting final  clean Data set for Model consumption."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd_Ac8F7SKPE",
        "outputId": "0957c571-efb7-4c3c-a9e7-e346862eceb3"
      },
      "source": [
        "# normal questions\n",
        "print(clean_df[clean_df['target'] == 0]['Text'][:5])\n",
        "# immature questions\n",
        "print(clean_df[clean_df['target'] == 1]['Text'][:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    how do quebec nationalist see their province a...\n",
            "1    do you have a adopt dog how would you encourag...\n",
            "2    why doe velocity affect time doe velocity affe...\n",
            "3    how do otto von guericke use the magdeburg hem...\n",
            "4    can i convert mantra helicon i to a mountain b...\n",
            "Name: Text, dtype: object\n",
            "22     have the unite state become the large dictator...\n",
            "30     which baby be much sweet to their parent dark ...\n",
            "110    if black support school choice and mandatory s...\n",
            "114    i be gay boy and i love my cousin boy he be se...\n",
            "115                      which race have the small penis\n",
            "Name: Text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGqQ2ykfVRH2"
      },
      "source": [
        "### Filtering out Sample Data\n",
        "\n",
        "The base Dataset is having category ratio  of 93:6, which makes distribution inbalance. Have resized the proportion 90:10 to reduce data imbalance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05hycwZFP-Pd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "0f69d407-4848-40be-9b68-6a04e7e39108"
      },
      "source": [
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "# Seperating insincere questions.\n",
        "#  Insincere Question \n",
        "insincere=clean_df[clean_df['target'] == 1]\n",
        "# Normal Question \n",
        "sincere=clean_df[clean_df['target'] == 0]\n",
        "\n",
        "\n",
        "#  Consuming 50% of insincere questions in train & test of Data\n",
        "top_50 = int(round(len(insincere)*0.5,0))\n",
        "\n",
        "# Generating seperate Dataframe with 30K \n",
        "insincere_train = insincere[:top_50]\n",
        "insincere_prod =  insincere[top_50:]\n",
        "\n",
        "sincere_train = sincere[:270000]\n",
        "sincere_prod = sincere[270000:]\n",
        "\n",
        "insincere_train = insincere_train.append(sincere_train)\n",
        "insincere_prod = insincere_prod.append(sincere_prod)\n",
        "\n",
        "\n",
        "train_shuffle = (list(random.sample(range(len(insincere_train)), len(insincere_train))))\n",
        "test_shuffle = (list(random.sample(range(len(insincere_prod)), len(insincere_prod))))\n",
        "\n",
        "\n",
        "insincere_train = insincere_train.iloc[train_shuffle,:]\n",
        "insincere_prod = insincere_prod.iloc[test_shuffle,:]\n",
        "\n",
        "del(sincere_train,sincere_prod)\n",
        "\n",
        "\n",
        "insincere_train.drop(['index','Rows'], axis=1, inplace= True)\n",
        "insincere_prod.drop(['index','Rows'], axis=1, inplace= True)\n",
        "\n",
        "print(insincere_train.shape, insincere_prod.shape)\n",
        "insincere_train.groupby('target').count()/ len(insincere_train) * 100"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300894, 2) (699104, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9e512496-e35b-49c3-a257-3947eb901b8a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>89.732597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.267403</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e512496-e35b-49c3-a257-3947eb901b8a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9e512496-e35b-49c3-a257-3947eb901b8a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9e512496-e35b-49c3-a257-3947eb901b8a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "             Text\n",
              "target           \n",
              "0       89.732597\n",
              "1       10.267403"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train & Test Split (60:40)"
      ],
      "metadata": {
        "id": "HYpjPe4V5PrB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fjze5nrnaiXZ",
        "outputId": "863d1002-6886-4f1d-d9c3-86efa6945915"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split Train and Validation data\n",
        "X_train, X_test = train_test_split( insincere_train, test_size=0.4, random_state=0)\n",
        "\n",
        "y_train = X_train['target']\n",
        "y_valid = X_test['target']\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(180536, 2) (120358, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7fKePMEbNKe"
      },
      "source": [
        "## Create a vocabulary index\n",
        "\n",
        "Let's use the `TextVectorization` to index the vocabulary found in the dataset.\n",
        "Later, we'll use the same layer instance to vectorize the samples.\n",
        "\n",
        "Our layer will only consider the top 30,000 words, and will truncate or pad sequences to be 40 tokens long.  We would do word embedding on  entire Data set. As model would have context of entire Dataset.\n",
        "\n",
        "Also below cell contains codes to save & reload the vectorizers, this would required for model deployment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evQVD0DwOYWG"
      },
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "vectorizer = TextVectorization(max_tokens=30000, output_sequence_length=40)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(X_train['Text']).batch(128)\n",
        "vectorizer.adapt(text_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7W_6NlFKOmR",
        "outputId": "36e5cd0c-d63b-461e-e04e-ed5d392cd93e"
      },
      "source": [
        "# Saving & loading the vectorizer again \n",
        "import pickle\n",
        "\n",
        "# Vector for word \"this\"\n",
        "print (vectorizer(\"this\"))\n",
        "\n",
        "# to save Pickle the config and weights\n",
        "pickle.dump({'config': vectorizer.get_config(),\n",
        "             'weights': vectorizer.get_weights()}\n",
        "            , open(\"vectorizer.pkl\", \"wb\"))\n",
        "\n",
        "print (\"*\"*100)\n",
        "\n",
        "from_disk = pickle.load(open(\"vectorizer.pkl\", \"rb\"))\n",
        "loaded_vector = TextVectorization.from_config(from_disk['config'])\n",
        "# You have to call `adapt` with some dummy data (BUG in Keras)\n",
        "loaded_vector.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
        "loaded_vector.set_weights(from_disk['weights'])\n",
        "\n",
        "\n",
        "s= [(loaded_vector(\"this\"))]\n",
        "kk = tf.keras.preprocessing.sequence.pad_sequences( s,maxlen= 40, padding='post')\n",
        "print(kk)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[55  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0], shape=(40,), dtype=int64)\n",
            "****************************************************************************************************\n",
            "[[55  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Glimpse of First 10 words vocabulary"
      ],
      "metadata": {
        "id": "LaBO7YiMCGsB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHptNe6MOZOX",
        "outputId": "22dfb7e4-0e41-4dc8-b0d7-089023101249"
      },
      "source": [
        "print(vectorizer.get_vocabulary()[:10])\n",
        "print(loaded_vector.get_vocabulary()[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', 'be', 'the', 'a', 'what', 'to', 'in', 'do', 'of']\n",
            "['', '[UNK]', 'be', 'the', 'a', 'what', 'to', 'in', 'do', 'of']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJSGXdhlP0bt"
      },
      "source": [
        "As you can see, \"be\" gets represented as \"2\". Why not 0, given that \"be\" was the first word in the vocabulary? That's because index 0 is reserved for padding and index 1 is reserved for \"out of vocabulary\" tokens.\n",
        "Creating a dict mapping words to their indices:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVO2Nx5UPvbW",
        "outputId": "4ec24947-3315-4525-ddec-740b2eeada64"
      },
      "source": [
        "# Creating vocabulary with index values\n",
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))\n",
        "# printing index of flower\n",
        "print(word_index['flower'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6GdlWJYP6ZR"
      },
      "source": [
        "As you can see, we obtain the same encoding as above for our test sentence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCwTeCTEP8UM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8c4456d-8e35-454d-9472-a57fca1c00a6"
      },
      "source": [
        "test = ['iran', 'life']\n",
        "[word_index[w] for w in test]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1230, 77]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ax139LMQ08O"
      },
      "source": [
        "\n",
        "#### Importing web embedding. \n",
        "We would be importing word embedding. The archive contains  text-encoded vectors of various sizes: 50-dimensional, 100-dimensional, 200-dimensional, 300-dimensional.  would be using 50D ones. Word embedding from glove has been used here. downloaded from [Glove](https://nlp.stanford.edu/projects/glove/)\n",
        "\n",
        "Let's make a dict mapping words (strings) to their NumPy vector representation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8x8eNvqQucx",
        "outputId": "c0a5348b-21d8-45b4-cfd2-40dd84bdd71e"
      },
      "source": [
        "path_to_glove_file = os.path.join(\n",
        "    os.path.expanduser(\"~\"), \"/content/drive/MyDrive/Word_Embeddings/glove.6B.50d.txt\"\n",
        ")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTw-GcRJShw6"
      },
      "source": [
        "### Embedding Matrix\n",
        "\n",
        "Now, let's prepare a corresponding embedding matrix that we can use in a Keras\n",
        "`Embedding` layer. It's a simple NumPy matrix where entry at index `i` is the pre-trained\n",
        "vector for the word of index `i` in our `vectorizer`'s vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlaq3oh4SdTS",
        "outputId": "c220dc1d-d3be-428c-a761-f5688d4ff31e"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 50\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 26097 words (3903 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48LzbXLhT0PD"
      },
      "source": [
        "### Evaluating if the Mapping of words has happend accurately for embedding matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlTe64TDTFUw",
        "outputId": "90999d54-3cf4-402e-adb0-aea5c24fdda3"
      },
      "source": [
        "# in Word glove dictionary embedding of Iran as an example\n",
        "print(embeddings_index['iran'])\n",
        "\n",
        "# indexing number in vocb for iran\n",
        "print(word_index['iran'])\n",
        "\n",
        "#  embedding matrix of iran through Numerical indexing \n",
        "print(embedding_matrix[1230])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.18997    0.11493    0.85566   -0.039811   0.10742   -0.44042\n",
            "  1.2496     0.49928    0.58689    0.8321     0.027948  -0.85445\n",
            " -0.39854   -0.18763   -0.050099   0.95036    0.59861    0.25454\n",
            "  0.6548     0.87505    0.82139   -0.0041283  0.9193    -0.033385\n",
            "  0.1914    -3.0393     0.58703    0.23673    0.031058   0.17775\n",
            "  2.4503    -0.35655   -0.68777   -0.43984    0.12271   -0.46345\n",
            " -0.29642    0.33648   -1.6442     0.23183   -0.019779   0.0057172\n",
            "  0.94701   -1.2708     0.53767    0.80297   -0.70422    1.7059\n",
            " -0.64729   -0.97299  ]\n",
            "1230\n",
            "[-0.18997     0.11493     0.85566002 -0.039811    0.10742    -0.44042\n",
            "  1.24960005  0.49928001  0.58688998  0.83209997  0.027948   -0.85444999\n",
            " -0.39853999 -0.18763    -0.050099    0.95036     0.59860998  0.25454\n",
            "  0.6548      0.87505001  0.82138997 -0.0041283   0.91930002 -0.033385\n",
            "  0.19140001 -3.03929996  0.58702999  0.23672999  0.031058    0.17775001\n",
            "  2.45029998 -0.35655001 -0.68777001 -0.43983999  0.12271    -0.46345001\n",
            " -0.29642001  0.33647999 -1.64419997  0.23183    -0.019779    0.0057172\n",
            "  0.94700998 -1.27079999  0.53767002  0.80296999 -0.70422     1.70589995\n",
            " -0.64728999 -0.97298998]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying Vectorizer (Numerical Indexing) on Train & Test Data."
      ],
      "metadata": {
        "id": "Pr4tPVcqIZVk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKnbvxamZE5V"
      },
      "source": [
        "x_train = vectorizer(np.array([[s] for s in X_train['Text']])).numpy()\n",
        "x_val = vectorizer(np.array([[s] for s in X_test['Text']])).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3U6ILDjS4ac"
      },
      "source": [
        "### Loading embedding Layer\n",
        "\n",
        "Next, we load the pre-trained word embeddings matrix into an `Embedding` layer.\n",
        "\n",
        "Note that we set `trainable=True` so as to fine tune the embedding as per contextual requirement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZdaQgQpV3I1"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monitoring Metrics for Epochs"
      ],
      "metadata": {
        "id": "GFt9nz4vJtY5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru6Sz-p5d2Wx"
      },
      "source": [
        "#https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFshfebgpGbo"
      },
      "source": [
        "### Model 1 - Simple Bi-LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVmyDoLuYeaU"
      },
      "source": [
        "#  Import K to clear session for model.\n",
        "from keras import backend as K\n",
        "K.clear_session()\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "# Define weight initializer with a random seed to ensure reproducibility\n",
        "weight_initializer = tf.keras.initializers.GlorotNormal(seed=RANDOM_STATE)\n",
        "\n",
        "def simple_LSTM():\n",
        "  K.clear_session()\n",
        "  model=Sequential()\n",
        "  model.add(embedding_layer)\n",
        "  model.add(Bidirectional(LSTM(64)))\n",
        "  # Adding Dropout\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add((Dense(64, activation= 'relu')))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(1, activation='sigmoid',\n",
        "                  kernel_initializer=weight_initializer))\n",
        "  # compile the model\n",
        "  optimzer = keras.optimizers.Adam(   #clipvalue=0.5,\n",
        "                                   learning_rate= 0.0001) # clip value to avoid the gradient exploding\n",
        " \n",
        "  model.compile(optimizer=optimzer, \n",
        "              loss='binary_crossentropy', \n",
        "              metrics=['acc',f1_m,precision_m, recall_m, tf.keras.metrics.AUC()])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b5BNdyhfD6S",
        "outputId": "9dbbdd2e-b909-4082-95f5-8364811d3986"
      },
      "source": [
        "# Model Initiation\n",
        "simple_LSTM_Model = simple_LSTM()\n",
        "\n",
        "# Batch Size \n",
        "BATCH_SIZE = 512 \n",
        "NUM_STEPS = len(X_train.index) // int(BATCH_SIZE)\n",
        "\n",
        "# early stopping\n",
        "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor= 'val_auc', \n",
        "                                                 patience=5,\n",
        "                                                 mode='max',\n",
        "                                                 restore_best_weights=True)\n",
        "# fit the model\n",
        "simple_LSTM_Model.fit(x_train, y_train,\n",
        "                      epochs=100,\n",
        "                      batch_size= BATCH_SIZE  ,\n",
        "                      steps_per_epoch = NUM_STEPS,\n",
        "                      callbacks=[earlyStopping], \n",
        "                      validation_data=(x_val,y_valid),\n",
        "                      verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "352/352 [==============================] - 8s 15ms/step - loss: 0.2487 - acc: 0.9158 - f1_m: 0.3786 - precision_m: 0.4960 - recall_m: 0.3573 - auc: 0.8570 - val_loss: 0.1699 - val_acc: 0.9315 - val_f1_m: 0.6601 - val_precision_m: 0.6652 - val_recall_m: 0.6596 - val_auc: 0.9419\n",
            "Epoch 2/100\n",
            "352/352 [==============================] - 5s 13ms/step - loss: 0.1275 - acc: 0.9480 - f1_m: 0.7462 - precision_m: 0.7491 - recall_m: 0.7491 - auc: 0.9678 - val_loss: 0.1795 - val_acc: 0.9315 - val_f1_m: 0.6655 - val_precision_m: 0.6604 - val_recall_m: 0.6747 - val_auc: 0.9323\n",
            "Epoch 3/100\n",
            "352/352 [==============================] - 5s 13ms/step - loss: 0.1118 - acc: 0.9550 - f1_m: 0.7822 - precision_m: 0.7799 - recall_m: 0.7900 - auc: 0.9753 - val_loss: 0.1888 - val_acc: 0.9290 - val_f1_m: 0.6572 - val_precision_m: 0.6443 - val_recall_m: 0.6753 - val_auc: 0.9257\n",
            "Epoch 4/100\n",
            "352/352 [==============================] - 5s 13ms/step - loss: 0.1074 - acc: 0.9569 - f1_m: 0.7906 - precision_m: 0.7871 - recall_m: 0.7993 - auc: 0.9771 - val_loss: 0.1958 - val_acc: 0.9303 - val_f1_m: 0.6518 - val_precision_m: 0.6618 - val_recall_m: 0.6464 - val_auc: 0.9213\n",
            "Epoch 5/100\n",
            "352/352 [==============================] - 5s 13ms/step - loss: 0.1049 - acc: 0.9581 - f1_m: 0.7964 - precision_m: 0.7891 - recall_m: 0.8088 - auc: 0.9782 - val_loss: 0.2006 - val_acc: 0.9300 - val_f1_m: 0.6493 - val_precision_m: 0.6605 - val_recall_m: 0.6429 - val_auc: 0.9189\n",
            "Epoch 6/100\n",
            "352/352 [==============================] - 5s 13ms/step - loss: 0.1032 - acc: 0.9589 - f1_m: 0.8012 - precision_m: 0.7944 - recall_m: 0.8135 - auc: 0.9788 - val_loss: 0.2017 - val_acc: 0.9271 - val_f1_m: 0.6510 - val_precision_m: 0.6328 - val_recall_m: 0.6748 - val_auc: 0.9196\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6844a8ead0>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model-1 Test predict Results  "
      ],
      "metadata": {
        "id": "mKgDlS7rMVP7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PNirmiX7lqN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e06221-05f9-42dc-e0d5-893cfee3b2a2"
      },
      "source": [
        "\n",
        "# model results\n",
        "train_pred = simple_LSTM_Model.predict(x_train)\n",
        "test_pred = simple_LSTM_Model.predict(x_val)\n",
        "\n",
        "train_pred_binary = np.where(train_pred> 0.49,1,0)\n",
        "test_pred_binary = np.where(test_pred> 0.49,1,0)\n",
        "# reshaping array \n",
        "train_pred_binary = train_pred_binary.reshape(180536,)\n",
        "test_pred_binary = test_pred_binary.reshape(120358,)\n",
        "\n",
        "\n",
        "# Accuracy\n",
        "acc = sum(train_pred_binary == y_train)/  len(y_train)\n",
        "test_acc = sum(test_pred_binary == y_valid)/  len(y_valid)\n",
        "print('Accuracy of train model is {}'.format(acc))\n",
        "print('Accuracy of test model is {}'.format(test_acc))\n",
        "\n",
        "\n",
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_valid,test_pred_binary))\n",
        "\n",
        "\n",
        "#  Recall  , f1 , precision \n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_valid,test_pred_binary))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of train model is 0.9427094873044711\n",
            "Accuracy of test model is 0.9311304607919706\n",
            "[[103889   4197]\n",
            " [  4092   8180]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.96      0.96    108086\n",
            "           1       0.66      0.67      0.66     12272\n",
            "\n",
            "    accuracy                           0.93    120358\n",
            "   macro avg       0.81      0.81      0.81    120358\n",
            "weighted avg       0.93      0.93      0.93    120358\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving Model 1 "
      ],
      "metadata": {
        "id": "v5n2qHC3OtTn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjBuc3jS2l5B"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Save the entire model to a HDF5 file.\n",
        "# The '.h5' extension indicates that the model should be saved to HDF5.\n",
        "model.save('model_1_final_20210911.h5') \n",
        "files.download('model_1_final_20210911.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qy2K0LpJ_uw"
      },
      "source": [
        "### MODEL 2 - Adding Dual Bi-LSTM Layer with GlobalAveragepooling1D \n",
        "\n",
        "GlobalAveragepooling1D is used to reduce dimentiality of data  in order to reduce noise from data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2wm7BQtUR_r"
      },
      "source": [
        "LSTM_UNITS = 64\n",
        "BATCH_SIZE = 512\n",
        "DENSE_HIDDEN_UNITS = 2 * LSTM_UNITS\n",
        "EPOCHS = 100\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=True)\n",
        "\n",
        "\n",
        "#  Import K to clear session for model.\n",
        "from keras import backend as K\n",
        "K.clear_session()\n",
        "\n",
        "\n",
        "def LSTM_with_Pooling():\n",
        "  K.clear_session()\n",
        "  model=Sequential()\n",
        "  model.add(embedding_layer)\n",
        "  model.add(Bidirectional(LSTM(LSTM_UNITS, return_sequences=True)))\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Bidirectional(LSTM(LSTM_UNITS, return_sequences=True)))\n",
        "  model.add( Dense(DENSE_HIDDEN_UNITS,  activation='relu') ) \n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(GlobalAveragePooling1D())\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  # compile the model\n",
        "  optimzer = keras.optimizers.Adam(    #clipvalue=0.5,\n",
        "                                   learning_rate= 0.0001) # clip value to avoid the gradient exploding\n",
        "  model.compile(optimizer=optimzer,\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['acc',f1_m,precision_m, recall_m, tf.keras.metrics.AUC()])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brDlW5vT-mT3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dc7da8a-93f4-4c08-a456-a90204acf10b"
      },
      "source": [
        "\n",
        "# early stopping\n",
        "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor= 'val_recall_m', \n",
        "                                                 patience=20,\n",
        "                                                 mode='max',\n",
        "                                                 restore_best_weights=True)\n",
        "\n",
        "model1 = LSTM_with_Pooling()\n",
        "model1.fit(x_train, y_train,\n",
        "                    epochs=EPOCHS, \n",
        "                    batch_size= BATCH_SIZE  ,\n",
        "                    callbacks=[earlyStopping], #check,\n",
        "                    validation_data=(x_val,y_valid), \n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "353/353 [==============================] - 19s 36ms/step - loss: 0.2829 - acc: 0.9089 - f1_m: 0.2821 - precision_m: 0.4095 - recall_m: 0.2409 - auc: 0.7965 - val_loss: 0.1812 - val_acc: 0.9258 - val_f1_m: 0.5975 - val_precision_m: 0.6649 - val_recall_m: 0.5470 - val_auc: 0.9308\n",
            "Epoch 2/100\n",
            "353/353 [==============================] - 10s 29ms/step - loss: 0.1754 - acc: 0.9278 - f1_m: 0.6076 - precision_m: 0.6901 - recall_m: 0.5490 - auc: 0.9365 - val_loss: 0.1741 - val_acc: 0.9289 - val_f1_m: 0.6451 - val_precision_m: 0.6552 - val_recall_m: 0.6403 - val_auc: 0.9392\n",
            "Epoch 3/100\n",
            "353/353 [==============================] - 10s 29ms/step - loss: 0.1669 - acc: 0.9313 - f1_m: 0.6313 - precision_m: 0.7070 - recall_m: 0.5781 - auc: 0.9433 - val_loss: 0.1689 - val_acc: 0.9324 - val_f1_m: 0.6225 - val_precision_m: 0.7137 - val_recall_m: 0.5557 - val_auc: 0.9407\n",
            "Epoch 4/100\n",
            "353/353 [==============================] - 10s 29ms/step - loss: 0.1605 - acc: 0.9342 - f1_m: 0.6510 - precision_m: 0.7172 - recall_m: 0.6023 - auc: 0.9476 - val_loss: 0.1676 - val_acc: 0.9333 - val_f1_m: 0.6164 - val_precision_m: 0.7352 - val_recall_m: 0.5345 - val_auc: 0.9425\n",
            "Epoch 5/100\n",
            "353/353 [==============================] - 10s 29ms/step - loss: 0.1554 - acc: 0.9363 - f1_m: 0.6655 - precision_m: 0.7237 - recall_m: 0.6224 - auc: 0.9513 - val_loss: 0.1604 - val_acc: 0.9351 - val_f1_m: 0.6569 - val_precision_m: 0.7076 - val_recall_m: 0.6172 - val_auc: 0.9463\n",
            "Epoch 6/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.1502 - acc: 0.9386 - f1_m: 0.6801 - precision_m: 0.7311 - recall_m: 0.6419 - auc: 0.9547 - val_loss: 0.1647 - val_acc: 0.9342 - val_f1_m: 0.6143 - val_precision_m: 0.7596 - val_recall_m: 0.5196 - val_auc: 0.9460\n",
            "Epoch 7/100\n",
            "353/353 [==============================] - 10s 29ms/step - loss: 0.1456 - acc: 0.9405 - f1_m: 0.6926 - precision_m: 0.7399 - recall_m: 0.6580 - auc: 0.9578 - val_loss: 0.1564 - val_acc: 0.9366 - val_f1_m: 0.6739 - val_precision_m: 0.7045 - val_recall_m: 0.6502 - val_auc: 0.9496\n",
            "Epoch 8/100\n",
            "353/353 [==============================] - 10s 29ms/step - loss: 0.1409 - acc: 0.9427 - f1_m: 0.7061 - precision_m: 0.7458 - recall_m: 0.6765 - auc: 0.9606 - val_loss: 0.1594 - val_acc: 0.9368 - val_f1_m: 0.6698 - val_precision_m: 0.7138 - val_recall_m: 0.6351 - val_auc: 0.9469\n",
            "Epoch 9/100\n",
            "353/353 [==============================] - 11s 30ms/step - loss: 0.1362 - acc: 0.9447 - f1_m: 0.7202 - precision_m: 0.7519 - recall_m: 0.6969 - auc: 0.9636 - val_loss: 0.1632 - val_acc: 0.9375 - val_f1_m: 0.6580 - val_precision_m: 0.7393 - val_recall_m: 0.5969 - val_auc: 0.9451\n",
            "Epoch 10/100\n",
            "353/353 [==============================] - 11s 30ms/step - loss: 0.1331 - acc: 0.9459 - f1_m: 0.7265 - precision_m: 0.7549 - recall_m: 0.7067 - auc: 0.9654 - val_loss: 0.1571 - val_acc: 0.9366 - val_f1_m: 0.6885 - val_precision_m: 0.6864 - val_recall_m: 0.6951 - val_auc: 0.9484\n",
            "Epoch 11/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.1289 - acc: 0.9474 - f1_m: 0.7363 - precision_m: 0.7597 - recall_m: 0.7200 - auc: 0.9678 - val_loss: 0.1577 - val_acc: 0.9369 - val_f1_m: 0.6850 - val_precision_m: 0.6948 - val_recall_m: 0.6806 - val_auc: 0.9476\n",
            "Epoch 12/100\n",
            "353/353 [==============================] - 10s 29ms/step - loss: 0.1259 - acc: 0.9488 - f1_m: 0.7430 - precision_m: 0.7640 - recall_m: 0.7298 - auc: 0.9693 - val_loss: 0.1585 - val_acc: 0.9368 - val_f1_m: 0.6875 - val_precision_m: 0.6902 - val_recall_m: 0.6897 - val_auc: 0.9470\n",
            "Epoch 13/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.1230 - acc: 0.9500 - f1_m: 0.7514 - precision_m: 0.7683 - recall_m: 0.7407 - auc: 0.9707 - val_loss: 0.1634 - val_acc: 0.9373 - val_f1_m: 0.6719 - val_precision_m: 0.7169 - val_recall_m: 0.6364 - val_auc: 0.9439\n",
            "Epoch 14/100\n",
            "353/353 [==============================] - 10s 29ms/step - loss: 0.1200 - acc: 0.9511 - f1_m: 0.7578 - precision_m: 0.7699 - recall_m: 0.7516 - auc: 0.9723 - val_loss: 0.1624 - val_acc: 0.9370 - val_f1_m: 0.6828 - val_precision_m: 0.6984 - val_recall_m: 0.6726 - val_auc: 0.9445\n",
            "Epoch 15/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.1179 - acc: 0.9520 - f1_m: 0.7635 - precision_m: 0.7737 - recall_m: 0.7596 - auc: 0.9731 - val_loss: 0.1682 - val_acc: 0.9372 - val_f1_m: 0.6573 - val_precision_m: 0.7382 - val_recall_m: 0.5961 - val_auc: 0.9409\n",
            "Epoch 16/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.1165 - acc: 0.9524 - f1_m: 0.7646 - precision_m: 0.7759 - recall_m: 0.7606 - auc: 0.9737 - val_loss: 0.1646 - val_acc: 0.9339 - val_f1_m: 0.6828 - val_precision_m: 0.6655 - val_recall_m: 0.7060 - val_auc: 0.9437\n",
            "Epoch 17/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.1141 - acc: 0.9538 - f1_m: 0.7721 - precision_m: 0.7823 - recall_m: 0.7698 - auc: 0.9748 - val_loss: 0.1740 - val_acc: 0.9371 - val_f1_m: 0.6660 - val_precision_m: 0.7231 - val_recall_m: 0.6216 - val_auc: 0.9363\n",
            "Epoch 18/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.1121 - acc: 0.9541 - f1_m: 0.7746 - precision_m: 0.7803 - recall_m: 0.7752 - auc: 0.9757 - val_loss: 0.1800 - val_acc: 0.9367 - val_f1_m: 0.6527 - val_precision_m: 0.7371 - val_recall_m: 0.5897 - val_auc: 0.9339\n",
            "Epoch 19/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.1098 - acc: 0.9555 - f1_m: 0.7819 - precision_m: 0.7851 - recall_m: 0.7842 - auc: 0.9766 - val_loss: 0.1742 - val_acc: 0.9347 - val_f1_m: 0.6740 - val_precision_m: 0.6835 - val_recall_m: 0.6693 - val_auc: 0.9361\n",
            "Epoch 20/100\n",
            "353/353 [==============================] - 11s 30ms/step - loss: 0.1084 - acc: 0.9559 - f1_m: 0.7853 - precision_m: 0.7858 - recall_m: 0.7912 - auc: 0.9774 - val_loss: 0.1782 - val_acc: 0.9358 - val_f1_m: 0.6687 - val_precision_m: 0.7032 - val_recall_m: 0.6420 - val_auc: 0.9340\n",
            "Epoch 21/100\n",
            "353/353 [==============================] - 10s 29ms/step - loss: 0.1064 - acc: 0.9568 - f1_m: 0.7900 - precision_m: 0.7894 - recall_m: 0.7959 - auc: 0.9781 - val_loss: 0.1752 - val_acc: 0.9348 - val_f1_m: 0.6749 - val_precision_m: 0.6837 - val_recall_m: 0.6707 - val_auc: 0.9365\n",
            "Epoch 22/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.1049 - acc: 0.9573 - f1_m: 0.7927 - precision_m: 0.7909 - recall_m: 0.8003 - auc: 0.9789 - val_loss: 0.1779 - val_acc: 0.9341 - val_f1_m: 0.6734 - val_precision_m: 0.6767 - val_recall_m: 0.6747 - val_auc: 0.9341\n",
            "Epoch 23/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.1040 - acc: 0.9578 - f1_m: 0.7952 - precision_m: 0.7930 - recall_m: 0.8043 - auc: 0.9790 - val_loss: 0.1850 - val_acc: 0.9248 - val_f1_m: 0.6686 - val_precision_m: 0.6053 - val_recall_m: 0.7519 - val_auc: 0.9359\n",
            "Epoch 24/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.1021 - acc: 0.9590 - f1_m: 0.8011 - precision_m: 0.7973 - recall_m: 0.8106 - auc: 0.9796 - val_loss: 0.1870 - val_acc: 0.9328 - val_f1_m: 0.6693 - val_precision_m: 0.6688 - val_recall_m: 0.6743 - val_auc: 0.9308\n",
            "Epoch 25/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.1006 - acc: 0.9596 - f1_m: 0.8046 - precision_m: 0.7981 - recall_m: 0.8160 - auc: 0.9801 - val_loss: 0.1889 - val_acc: 0.9277 - val_f1_m: 0.6719 - val_precision_m: 0.6229 - val_recall_m: 0.7345 - val_auc: 0.9316\n",
            "Epoch 26/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.1000 - acc: 0.9599 - f1_m: 0.8065 - precision_m: 0.7996 - recall_m: 0.8198 - auc: 0.9803 - val_loss: 0.1873 - val_acc: 0.9328 - val_f1_m: 0.6669 - val_precision_m: 0.6706 - val_recall_m: 0.6676 - val_auc: 0.9301\n",
            "Epoch 27/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.0981 - acc: 0.9607 - f1_m: 0.8109 - precision_m: 0.8027 - recall_m: 0.8249 - auc: 0.9812 - val_loss: 0.1914 - val_acc: 0.9287 - val_f1_m: 0.6688 - val_precision_m: 0.6324 - val_recall_m: 0.7147 - val_auc: 0.9287\n",
            "Epoch 28/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.0973 - acc: 0.9609 - f1_m: 0.8125 - precision_m: 0.8021 - recall_m: 0.8285 - auc: 0.9812 - val_loss: 0.1942 - val_acc: 0.9331 - val_f1_m: 0.6657 - val_precision_m: 0.6748 - val_recall_m: 0.6613 - val_auc: 0.9256\n",
            "Epoch 29/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.0962 - acc: 0.9618 - f1_m: 0.8158 - precision_m: 0.8080 - recall_m: 0.8294 - auc: 0.9817 - val_loss: 0.1920 - val_acc: 0.9299 - val_f1_m: 0.6663 - val_precision_m: 0.6433 - val_recall_m: 0.6959 - val_auc: 0.9271\n",
            "Epoch 30/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.0953 - acc: 0.9616 - f1_m: 0.8159 - precision_m: 0.8044 - recall_m: 0.8338 - auc: 0.9819 - val_loss: 0.1960 - val_acc: 0.9315 - val_f1_m: 0.6682 - val_precision_m: 0.6568 - val_recall_m: 0.6846 - val_auc: 0.9253\n",
            "Epoch 31/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.0936 - acc: 0.9626 - f1_m: 0.8202 - precision_m: 0.8089 - recall_m: 0.8367 - auc: 0.9824 - val_loss: 0.2021 - val_acc: 0.9274 - val_f1_m: 0.6635 - val_precision_m: 0.6267 - val_recall_m: 0.7102 - val_auc: 0.9239\n",
            "Epoch 32/100\n",
            "353/353 [==============================] - 11s 30ms/step - loss: 0.0925 - acc: 0.9631 - f1_m: 0.8230 - precision_m: 0.8080 - recall_m: 0.8434 - auc: 0.9826 - val_loss: 0.2093 - val_acc: 0.9330 - val_f1_m: 0.6501 - val_precision_m: 0.6922 - val_recall_m: 0.6173 - val_auc: 0.9163\n",
            "Epoch 33/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.0922 - acc: 0.9632 - f1_m: 0.8243 - precision_m: 0.8097 - recall_m: 0.8453 - auc: 0.9829 - val_loss: 0.2082 - val_acc: 0.9337 - val_f1_m: 0.6476 - val_precision_m: 0.7037 - val_recall_m: 0.6042 - val_auc: 0.9176\n",
            "Epoch 34/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.0906 - acc: 0.9641 - f1_m: 0.8286 - precision_m: 0.8134 - recall_m: 0.8492 - auc: 0.9833 - val_loss: 0.2087 - val_acc: 0.9317 - val_f1_m: 0.6564 - val_precision_m: 0.6713 - val_recall_m: 0.6469 - val_auc: 0.9176\n",
            "Epoch 35/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.0892 - acc: 0.9649 - f1_m: 0.8326 - precision_m: 0.8148 - recall_m: 0.8546 - auc: 0.9838 - val_loss: 0.2093 - val_acc: 0.9274 - val_f1_m: 0.6589 - val_precision_m: 0.6298 - val_recall_m: 0.6960 - val_auc: 0.9198\n",
            "Epoch 36/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.0900 - acc: 0.9648 - f1_m: 0.8310 - precision_m: 0.8168 - recall_m: 0.8523 - auc: 0.9837 - val_loss: 0.2084 - val_acc: 0.9325 - val_f1_m: 0.6436 - val_precision_m: 0.6938 - val_recall_m: 0.6047 - val_auc: 0.9167\n",
            "Epoch 37/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.0882 - acc: 0.9655 - f1_m: 0.8352 - precision_m: 0.8208 - recall_m: 0.8552 - auc: 0.9840 - val_loss: 0.2166 - val_acc: 0.9243 - val_f1_m: 0.6563 - val_precision_m: 0.6090 - val_recall_m: 0.7170 - val_auc: 0.9185\n",
            "Epoch 38/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.0868 - acc: 0.9660 - f1_m: 0.8378 - precision_m: 0.8185 - recall_m: 0.8623 - auc: 0.9846 - val_loss: 0.2178 - val_acc: 0.9241 - val_f1_m: 0.6547 - val_precision_m: 0.6086 - val_recall_m: 0.7136 - val_auc: 0.9184\n",
            "Epoch 39/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.0869 - acc: 0.9658 - f1_m: 0.8373 - precision_m: 0.8194 - recall_m: 0.8611 - auc: 0.9844 - val_loss: 0.2181 - val_acc: 0.9281 - val_f1_m: 0.6536 - val_precision_m: 0.6401 - val_recall_m: 0.6728 - val_auc: 0.9163\n",
            "Epoch 40/100\n",
            "353/353 [==============================] - 11s 30ms/step - loss: 0.0854 - acc: 0.9664 - f1_m: 0.8401 - precision_m: 0.8192 - recall_m: 0.8667 - auc: 0.9847 - val_loss: 0.2206 - val_acc: 0.9303 - val_f1_m: 0.6396 - val_precision_m: 0.6740 - val_recall_m: 0.6134 - val_auc: 0.9100\n",
            "Epoch 41/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.0848 - acc: 0.9670 - f1_m: 0.8424 - precision_m: 0.8244 - recall_m: 0.8658 - auc: 0.9851 - val_loss: 0.2133 - val_acc: 0.9305 - val_f1_m: 0.6475 - val_precision_m: 0.6674 - val_recall_m: 0.6337 - val_auc: 0.9138\n",
            "Epoch 42/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.0842 - acc: 0.9669 - f1_m: 0.8430 - precision_m: 0.8238 - recall_m: 0.8677 - auc: 0.9851 - val_loss: 0.2325 - val_acc: 0.9258 - val_f1_m: 0.6459 - val_precision_m: 0.6264 - val_recall_m: 0.6721 - val_auc: 0.9086\n",
            "Epoch 43/100\n",
            "353/353 [==============================] - 10s 30ms/step - loss: 0.0833 - acc: 0.9674 - f1_m: 0.8451 - precision_m: 0.8257 - recall_m: 0.8701 - auc: 0.9854 - val_loss: 0.2279 - val_acc: 0.9272 - val_f1_m: 0.6503 - val_precision_m: 0.6344 - val_recall_m: 0.6723 - val_auc: 0.9115\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa5c4675950>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NikwyWhEbec"
      },
      "source": [
        "### Saving  Model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxr1qH8l5Vo1",
        "outputId": "b8fc86d6-df67-4910-d232-a2a3cf45ab0c"
      },
      "source": [
        "### Saving Distilbert Model\n",
        "\n",
        "#  Saving Model weights\n",
        "model1.save_weights('LSTM_pooling_final/my_model')\n",
        "\n",
        "### Loading saved model.\n",
        "# loading the model params on which it was trained\n",
        "load_model = LSTM_with_Pooling()\n",
        "\n",
        "#  loading the training weights back to model.\n",
        "load_model.load_weights('LSTM_pooling_final/my_model')\n",
        "\n",
        "from google.colab import files\n",
        "!zip -r /content/LSTM_pooling_final.zip /content/LSTM_pooling_final\n",
        "files.download('LSTM_pooling_final.zip')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f6824fbb110>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Model 2 on validation data."
      ],
      "metadata": {
        "id": "lSGH3_zuQx5b"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbV4y8MlVOOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7035b96b-fbf5-41f1-9b04-963f3f03a622"
      },
      "source": [
        "# model results\n",
        "train_pred1 = load_model.predict(x_train)\n",
        "test_pred1 = load_model.predict(x_val)\n",
        "\n",
        "print(train_pred1.shape, test_pred1.shape)\n",
        "\n",
        "train_pred_binary1 = np.where(train_pred1> 0.49,1,0)\n",
        "test_pred_binary1 = np.where(test_pred1> 0.49,1,0)\n",
        "# reshaping array \n",
        "train_pred_binary1 = train_pred_binary1.reshape(180536,)\n",
        "test_pred_binary1 = test_pred_binary1.reshape(120358,)\n",
        "\n",
        "\n",
        "# Accuracy\n",
        "acc = sum(train_pred_binary1 == y_train)/  len(y_train)\n",
        "test_acc = sum(test_pred_binary1 == y_valid)/  len(y_valid)\n",
        "print('Accuracy of train model is {}'.format(acc))\n",
        "print('Accuracy of test model is {}'.format(test_acc))\n",
        "\n",
        "\n",
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_valid,test_pred_binary1))\n",
        "\n",
        "\n",
        "#  Recall  , f1 , precision \n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_valid,test_pred_binary1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of train model is 0.9421721983427128\n",
            "Accuracy of test model is 0.9333571511656891\n",
            "[[103495   4591]\n",
            " [  3430   8842]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.96      0.96    108086\n",
            "           1       0.66      0.72      0.69     12272\n",
            "\n",
            "    accuracy                           0.93    120358\n",
            "   macro avg       0.81      0.84      0.83    120358\n",
            "weighted avg       0.94      0.93      0.93    120358\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Sample Text to test model 2 results."
      ],
      "metadata": {
        "id": "52_yT2MoRjfn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88zBYvhc4L-5"
      },
      "source": [
        "ques = ['will democrats win 2021', \n",
        "        'is he still alive', \n",
        "        'name famous black celebreties',\n",
        "        'are republican not racists', \n",
        "        'distance to moon'\n",
        "        'which race is the smartest', \n",
        "        'what are achivements by trump' ,\n",
        "        'are republican racists',\n",
        "        'is my job safe', \n",
        "        'are jews loyal', \n",
        "        'which race is not loyal',\n",
        "        'why republican cry always', \n",
        "        'does trump hate gay', \n",
        "        'do we require another pandamic',\n",
        "        'is america under dictatorship', \n",
        "        'are muslim terrorist', \n",
        "        'do you support taliban',\n",
        "        'are white people', \n",
        "        'are you fool', \n",
        "        'are muslim extremist', \n",
        "        'is china facist country' ,\n",
        "        'are democrats dumb', \n",
        "        'do blacks support gay',  \n",
        "        'do americans support holocaust']\n",
        "ques = preprocess_parallel(ques)\n",
        "\n",
        "' '.join(ques[0])\n",
        "# ques = ' '.join(preprocess_parallel(ques))\n",
        "\n",
        "join_corpus =[]\n",
        "for l in ques:\n",
        "  join_corpus.append(' '.join(l))\n",
        "\n",
        "\n",
        "testing = vectorizer(np.array([[s] for s in join_corpus])).numpy()\n",
        "\n",
        "(model1.predict(testing))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRFGZxol_o3c"
      },
      "source": [
        "### Model 3 - Hybrid model LSTM + Conv1D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCU_8vc7CzKc"
      },
      "source": [
        "\n",
        "# Pointers\n",
        "LSTM_UNITS = 64\n",
        "BATCH_SIZE = 128\n",
        "DENSE_HIDDEN_UNITS = 2 * LSTM_UNITS\n",
        "EPOCHS = 100\n",
        "\n",
        "\n",
        "#  Import K to clear session for model.\n",
        "from keras import backend as K\n",
        "\n",
        "# Padding of sentence is done 40\n",
        "maxlen = 40 \n",
        "\n",
        "# Model 3\n",
        "def BiLSTM_CNN(spatialdropout=0.2, rnn_units=128, filters=[100, 80, 30, 12], weight_decay=0.10):\n",
        "  K.clear_session()\n",
        "  x_input = Input(shape=(maxlen,))\n",
        "  \n",
        "  emb = Embedding(num_tokens,\n",
        "                  embedding_dim, \n",
        "                  embeddings_initializer=keras.initializers.Constant(embedding_matrix), \n",
        "                  trainable=False, name='Embedding')(x_input)\n",
        "\n",
        "  # adding  spatial drop out will nullify the embedding vectors with Zeros.\n",
        "  x = SpatialDropout1D(rate=spatialdropout, seed=10000)(emb) \n",
        "\n",
        "  rnn = Bidirectional(LSTM(rnn_units, return_sequences=True, kernel_initializer=initializers.glorot_uniform(seed=123000), recurrent_initializer=initializers.Orthogonal(gain=1.0, seed=123000)))(x)\n",
        "  \n",
        "  # Adding 4 Channels of Conv1D layers\n",
        "  x1 = Conv1D(filters=filters[0], activation='relu', kernel_size=1, padding='same', kernel_initializer=initializers.glorot_uniform(seed=110000))(rnn)\n",
        "  x2 = Conv1D(filters=filters[1], activation='relu', kernel_size=1, padding='same', kernel_initializer=initializers.glorot_uniform(seed=120000))(rnn)\n",
        "  x3 = Conv1D(filters=filters[2], activation='relu', kernel_size=1, padding='same', kernel_initializer=initializers.glorot_uniform(seed=130000))(rnn)\n",
        "  x4 = Conv1D(filters=filters[3], activation='relu', kernel_size=1, padding='same', kernel_initializer=initializers.glorot_uniform(seed=140000))(rnn)\n",
        "\n",
        "# Adding max pooling to select domnant feature\n",
        "  x1 = GlobalMaxPooling1D()(x1)\n",
        "  x2 = GlobalMaxPooling1D()(x2)\n",
        "  x3 = GlobalMaxPooling1D()(x3)\n",
        "  x4 = GlobalMaxPooling1D()(x4)\n",
        "\n",
        "  c = concatenate([x1, x2, x3, x4])\n",
        "  x = Dense(256, activation='relu', kernel_initializer=initializers.glorot_uniform(seed=111000))(c)\n",
        "  x = Dropout(0.2, seed=10000)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x_output = Dense(1, activation='sigmoid', kernel_initializer=initializers.glorot_uniform(seed=110000))(x)\n",
        " \n",
        "  model = Model(inputs=x_input, outputs=x_output)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(    #clipvalue=0.5, \n",
        "                                 learning_rate= 0.0001) # clip value to avoid the gradient exploding\n",
        "                , metrics=['acc',f1_m,precision_m, recall_m, tf.keras.metrics.AUC()])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = BiLSTM_CNN()\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1alF0vy3eb5",
        "outputId": "5eb2642b-247b-444f-932f-e8f4900a77a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 40)]         0           []                               \n",
            "                                                                                                  \n",
            " Embedding (Embedding)          (None, 40, 50)       1500100     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " spatial_dropout1d (SpatialDrop  (None, 40, 50)      0           ['Embedding[0][0]']              \n",
            " out1D)                                                                                           \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 40, 256)      183296      ['spatial_dropout1d[0][0]']      \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 40, 100)      25700       ['bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 40, 80)       20560       ['bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 40, 30)       7710        ['bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 40, 12)       3084        ['bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 100)         0           ['conv1d[0][0]']                 \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " global_max_pooling1d_1 (Global  (None, 80)          0           ['conv1d_1[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " global_max_pooling1d_2 (Global  (None, 30)          0           ['conv1d_2[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " global_max_pooling1d_3 (Global  (None, 12)          0           ['conv1d_3[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 222)          0           ['global_max_pooling1d[0][0]',   \n",
            "                                                                  'global_max_pooling1d_1[0][0]', \n",
            "                                                                  'global_max_pooling1d_2[0][0]', \n",
            "                                                                  'global_max_pooling1d_3[0][0]'] \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 256)          57088       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 256)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 256)         1024        ['dropout[0][0]']                \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            257         ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,798,819\n",
            "Trainable params: 298,207\n",
            "Non-trainable params: 1,500,612\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ii1GAFNDDs91",
        "outputId": "d8d61032-d71f-46c6-9497-3d70152fbf08"
      },
      "source": [
        "# \n",
        "model2 = BiLSTM_CNN()\n",
        "\n",
        "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor= 'val_auc', \n",
        "                                                 patience=20,\n",
        "                                                 mode='max',\n",
        "                                                 restore_best_weights=True)\n",
        "\n",
        "model2.fit(x_train, y_train,\n",
        "                    epochs=EPOCHS, \n",
        "                    batch_size= BATCH_SIZE  ,\n",
        "                    callbacks=[earlyStopping], #check,\n",
        "                    validation_data=(x_val,y_valid), \n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1411/1411 [==============================] - 37s 17ms/step - loss: 0.3563 - acc: 0.8570 - f1_m: 0.4421 - precision_m: 0.4847 - recall_m: 0.5044 - auc: 0.8097 - val_loss: 0.2318 - val_acc: 0.9173 - val_f1_m: 0.5981 - val_precision_m: 0.5903 - val_recall_m: 0.6281 - val_auc: 0.9197\n",
            "Epoch 2/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.2051 - acc: 0.9197 - f1_m: 0.5093 - precision_m: 0.6786 - recall_m: 0.4226 - auc: 0.9083 - val_loss: 0.1837 - val_acc: 0.9304 - val_f1_m: 0.6132 - val_precision_m: 0.6936 - val_recall_m: 0.5670 - val_auc: 0.9338\n",
            "Epoch 3/100\n",
            "1411/1411 [==============================] - 23s 17ms/step - loss: 0.1913 - acc: 0.9230 - f1_m: 0.5425 - precision_m: 0.6935 - recall_m: 0.4616 - auc: 0.9219 - val_loss: 0.1731 - val_acc: 0.9319 - val_f1_m: 0.6169 - val_precision_m: 0.7070 - val_recall_m: 0.5639 - val_auc: 0.9370\n",
            "Epoch 4/100\n",
            "1411/1411 [==============================] - 23s 17ms/step - loss: 0.1837 - acc: 0.9262 - f1_m: 0.5684 - precision_m: 0.7090 - recall_m: 0.4918 - auc: 0.9282 - val_loss: 0.1696 - val_acc: 0.9335 - val_f1_m: 0.6379 - val_precision_m: 0.7019 - val_recall_m: 0.6027 - val_auc: 0.9408\n",
            "Epoch 5/100\n",
            "1411/1411 [==============================] - 23s 17ms/step - loss: 0.1800 - acc: 0.9281 - f1_m: 0.5846 - precision_m: 0.7122 - recall_m: 0.5134 - auc: 0.9315 - val_loss: 0.1666 - val_acc: 0.9341 - val_f1_m: 0.6545 - val_precision_m: 0.6902 - val_recall_m: 0.6405 - val_auc: 0.9432\n",
            "Epoch 6/100\n",
            "1411/1411 [==============================] - 23s 17ms/step - loss: 0.1762 - acc: 0.9295 - f1_m: 0.5971 - precision_m: 0.7200 - recall_m: 0.5286 - auc: 0.9346 - val_loss: 0.1667 - val_acc: 0.9343 - val_f1_m: 0.6633 - val_precision_m: 0.6823 - val_recall_m: 0.6624 - val_auc: 0.9449\n",
            "Epoch 7/100\n",
            "1411/1411 [==============================] - 23s 17ms/step - loss: 0.1733 - acc: 0.9306 - f1_m: 0.6073 - precision_m: 0.7233 - recall_m: 0.5414 - auc: 0.9368 - val_loss: 0.1822 - val_acc: 0.9291 - val_f1_m: 0.6707 - val_precision_m: 0.6301 - val_recall_m: 0.7363 - val_auc: 0.9454\n",
            "Epoch 8/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1706 - acc: 0.9318 - f1_m: 0.6162 - precision_m: 0.7256 - recall_m: 0.5542 - auc: 0.9388 - val_loss: 0.1795 - val_acc: 0.9291 - val_f1_m: 0.6728 - val_precision_m: 0.6286 - val_recall_m: 0.7435 - val_auc: 0.9459\n",
            "Epoch 9/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1686 - acc: 0.9328 - f1_m: 0.6242 - precision_m: 0.7277 - recall_m: 0.5655 - auc: 0.9405 - val_loss: 0.1643 - val_acc: 0.9354 - val_f1_m: 0.6703 - val_precision_m: 0.6876 - val_recall_m: 0.6712 - val_auc: 0.9469\n",
            "Epoch 10/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1665 - acc: 0.9336 - f1_m: 0.6297 - precision_m: 0.7305 - recall_m: 0.5710 - auc: 0.9419 - val_loss: 0.1591 - val_acc: 0.9368 - val_f1_m: 0.6433 - val_precision_m: 0.7398 - val_recall_m: 0.5856 - val_auc: 0.9474\n",
            "Epoch 11/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1646 - acc: 0.9342 - f1_m: 0.6356 - precision_m: 0.7325 - recall_m: 0.5790 - auc: 0.9437 - val_loss: 0.1582 - val_acc: 0.9374 - val_f1_m: 0.6714 - val_precision_m: 0.7098 - val_recall_m: 0.6540 - val_auc: 0.9478\n",
            "Epoch 12/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1623 - acc: 0.9353 - f1_m: 0.6427 - precision_m: 0.7370 - recall_m: 0.5869 - auc: 0.9451 - val_loss: 0.1622 - val_acc: 0.9363 - val_f1_m: 0.6799 - val_precision_m: 0.6867 - val_recall_m: 0.6901 - val_auc: 0.9484\n",
            "Epoch 13/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1613 - acc: 0.9355 - f1_m: 0.6445 - precision_m: 0.7387 - recall_m: 0.5883 - auc: 0.9461 - val_loss: 0.1613 - val_acc: 0.9365 - val_f1_m: 0.6684 - val_precision_m: 0.7038 - val_recall_m: 0.6533 - val_auc: 0.9487\n",
            "Epoch 14/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1586 - acc: 0.9363 - f1_m: 0.6490 - precision_m: 0.7399 - recall_m: 0.5955 - auc: 0.9479 - val_loss: 0.1557 - val_acc: 0.9384 - val_f1_m: 0.6741 - val_precision_m: 0.7174 - val_recall_m: 0.6518 - val_auc: 0.9499\n",
            "Epoch 15/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1580 - acc: 0.9368 - f1_m: 0.6529 - precision_m: 0.7415 - recall_m: 0.6013 - auc: 0.9482 - val_loss: 0.1672 - val_acc: 0.9355 - val_f1_m: 0.6862 - val_precision_m: 0.6714 - val_recall_m: 0.7187 - val_auc: 0.9502\n",
            "Epoch 16/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1560 - acc: 0.9377 - f1_m: 0.6605 - precision_m: 0.7462 - recall_m: 0.6106 - auc: 0.9496 - val_loss: 0.1678 - val_acc: 0.9331 - val_f1_m: 0.6865 - val_precision_m: 0.6500 - val_recall_m: 0.7457 - val_auc: 0.9502\n",
            "Epoch 17/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1557 - acc: 0.9377 - f1_m: 0.6594 - precision_m: 0.7420 - recall_m: 0.6119 - auc: 0.9500 - val_loss: 0.1558 - val_acc: 0.9382 - val_f1_m: 0.6567 - val_precision_m: 0.7421 - val_recall_m: 0.6046 - val_auc: 0.9491\n",
            "Epoch 18/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1537 - acc: 0.9386 - f1_m: 0.6652 - precision_m: 0.7478 - recall_m: 0.6174 - auc: 0.9514 - val_loss: 0.1567 - val_acc: 0.9378 - val_f1_m: 0.6804 - val_precision_m: 0.7007 - val_recall_m: 0.6779 - val_auc: 0.9497\n",
            "Epoch 19/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1525 - acc: 0.9394 - f1_m: 0.6714 - precision_m: 0.7500 - recall_m: 0.6268 - auc: 0.9524 - val_loss: 0.1557 - val_acc: 0.9384 - val_f1_m: 0.6825 - val_precision_m: 0.7056 - val_recall_m: 0.6770 - val_auc: 0.9505\n",
            "Epoch 20/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1507 - acc: 0.9402 - f1_m: 0.6751 - precision_m: 0.7524 - recall_m: 0.6290 - auc: 0.9536 - val_loss: 0.1767 - val_acc: 0.9297 - val_f1_m: 0.6849 - val_precision_m: 0.6240 - val_recall_m: 0.7784 - val_auc: 0.9509\n",
            "Epoch 21/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1490 - acc: 0.9409 - f1_m: 0.6793 - precision_m: 0.7554 - recall_m: 0.6342 - auc: 0.9548 - val_loss: 0.1635 - val_acc: 0.9350 - val_f1_m: 0.6872 - val_precision_m: 0.6653 - val_recall_m: 0.7290 - val_auc: 0.9509\n",
            "Epoch 22/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1490 - acc: 0.9403 - f1_m: 0.6795 - precision_m: 0.7544 - recall_m: 0.6361 - auc: 0.9545 - val_loss: 0.1576 - val_acc: 0.9370 - val_f1_m: 0.6863 - val_precision_m: 0.6880 - val_recall_m: 0.7025 - val_auc: 0.9517\n",
            "Epoch 23/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1478 - acc: 0.9410 - f1_m: 0.6807 - precision_m: 0.7563 - recall_m: 0.6370 - auc: 0.9555 - val_loss: 0.1698 - val_acc: 0.9323 - val_f1_m: 0.6878 - val_precision_m: 0.6422 - val_recall_m: 0.7592 - val_auc: 0.9515\n",
            "Epoch 24/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1462 - acc: 0.9415 - f1_m: 0.6844 - precision_m: 0.7574 - recall_m: 0.6405 - auc: 0.9563 - val_loss: 0.1597 - val_acc: 0.9368 - val_f1_m: 0.6910 - val_precision_m: 0.6788 - val_recall_m: 0.7205 - val_auc: 0.9512\n",
            "Epoch 25/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1451 - acc: 0.9421 - f1_m: 0.6882 - precision_m: 0.7601 - recall_m: 0.6467 - auc: 0.9571 - val_loss: 0.1673 - val_acc: 0.9332 - val_f1_m: 0.6904 - val_precision_m: 0.6471 - val_recall_m: 0.7589 - val_auc: 0.9514\n",
            "Epoch 26/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1437 - acc: 0.9428 - f1_m: 0.6930 - precision_m: 0.7616 - recall_m: 0.6521 - auc: 0.9579 - val_loss: 0.1580 - val_acc: 0.9372 - val_f1_m: 0.6863 - val_precision_m: 0.6901 - val_recall_m: 0.7007 - val_auc: 0.9510\n",
            "Epoch 27/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1420 - acc: 0.9436 - f1_m: 0.6975 - precision_m: 0.7665 - recall_m: 0.6557 - auc: 0.9589 - val_loss: 0.1554 - val_acc: 0.9386 - val_f1_m: 0.6797 - val_precision_m: 0.7152 - val_recall_m: 0.6647 - val_auc: 0.9506\n",
            "Epoch 28/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1418 - acc: 0.9436 - f1_m: 0.6972 - precision_m: 0.7652 - recall_m: 0.6568 - auc: 0.9593 - val_loss: 0.1546 - val_acc: 0.9390 - val_f1_m: 0.6675 - val_precision_m: 0.7373 - val_recall_m: 0.6264 - val_auc: 0.9495\n",
            "Epoch 29/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1398 - acc: 0.9441 - f1_m: 0.7019 - precision_m: 0.7686 - recall_m: 0.6632 - auc: 0.9604 - val_loss: 0.1573 - val_acc: 0.9376 - val_f1_m: 0.6816 - val_precision_m: 0.6992 - val_recall_m: 0.6816 - val_auc: 0.9499\n",
            "Epoch 30/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1388 - acc: 0.9446 - f1_m: 0.7030 - precision_m: 0.7728 - recall_m: 0.6621 - auc: 0.9613 - val_loss: 0.1579 - val_acc: 0.9376 - val_f1_m: 0.6878 - val_precision_m: 0.6912 - val_recall_m: 0.7006 - val_auc: 0.9507\n",
            "Epoch 31/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1381 - acc: 0.9447 - f1_m: 0.7048 - precision_m: 0.7682 - recall_m: 0.6676 - auc: 0.9616 - val_loss: 0.1551 - val_acc: 0.9381 - val_f1_m: 0.6787 - val_precision_m: 0.7093 - val_recall_m: 0.6665 - val_auc: 0.9507\n",
            "Epoch 32/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1368 - acc: 0.9450 - f1_m: 0.7072 - precision_m: 0.7678 - recall_m: 0.6726 - auc: 0.9625 - val_loss: 0.1622 - val_acc: 0.9357 - val_f1_m: 0.6923 - val_precision_m: 0.6678 - val_recall_m: 0.7365 - val_auc: 0.9514\n",
            "Epoch 33/100\n",
            "1411/1411 [==============================] - 23s 17ms/step - loss: 0.1357 - acc: 0.9455 - f1_m: 0.7107 - precision_m: 0.7726 - recall_m: 0.6743 - auc: 0.9629 - val_loss: 0.1548 - val_acc: 0.9383 - val_f1_m: 0.6812 - val_precision_m: 0.7088 - val_recall_m: 0.6733 - val_auc: 0.9514\n",
            "Epoch 34/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1345 - acc: 0.9464 - f1_m: 0.7142 - precision_m: 0.7767 - recall_m: 0.6780 - auc: 0.9636 - val_loss: 0.1787 - val_acc: 0.9291 - val_f1_m: 0.6873 - val_precision_m: 0.6189 - val_recall_m: 0.7927 - val_auc: 0.9519\n",
            "Epoch 35/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1337 - acc: 0.9467 - f1_m: 0.7178 - precision_m: 0.7763 - recall_m: 0.6846 - auc: 0.9641 - val_loss: 0.1611 - val_acc: 0.9360 - val_f1_m: 0.6929 - val_precision_m: 0.6695 - val_recall_m: 0.7360 - val_auc: 0.9517\n",
            "Epoch 36/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1337 - acc: 0.9462 - f1_m: 0.7155 - precision_m: 0.7763 - recall_m: 0.6817 - auc: 0.9639 - val_loss: 0.1553 - val_acc: 0.9392 - val_f1_m: 0.6723 - val_precision_m: 0.7331 - val_recall_m: 0.6371 - val_auc: 0.9491\n",
            "Epoch 37/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1315 - acc: 0.9480 - f1_m: 0.7253 - precision_m: 0.7835 - recall_m: 0.6918 - auc: 0.9652 - val_loss: 0.1579 - val_acc: 0.9389 - val_f1_m: 0.6867 - val_precision_m: 0.7076 - val_recall_m: 0.6835 - val_auc: 0.9501\n",
            "Epoch 38/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1304 - acc: 0.9472 - f1_m: 0.7202 - precision_m: 0.7762 - recall_m: 0.6881 - auc: 0.9661 - val_loss: 0.1604 - val_acc: 0.9378 - val_f1_m: 0.6879 - val_precision_m: 0.6944 - val_recall_m: 0.6981 - val_auc: 0.9486\n",
            "Epoch 39/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1293 - acc: 0.9482 - f1_m: 0.7258 - precision_m: 0.7835 - recall_m: 0.6930 - auc: 0.9666 - val_loss: 0.1628 - val_acc: 0.9370 - val_f1_m: 0.6948 - val_precision_m: 0.6784 - val_recall_m: 0.7294 - val_auc: 0.9506\n",
            "Epoch 40/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1285 - acc: 0.9481 - f1_m: 0.7254 - precision_m: 0.7821 - recall_m: 0.6934 - auc: 0.9673 - val_loss: 0.1619 - val_acc: 0.9366 - val_f1_m: 0.6871 - val_precision_m: 0.6811 - val_recall_m: 0.7104 - val_auc: 0.9496\n",
            "Epoch 41/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1283 - acc: 0.9490 - f1_m: 0.7306 - precision_m: 0.7853 - recall_m: 0.6983 - auc: 0.9672 - val_loss: 0.1632 - val_acc: 0.9359 - val_f1_m: 0.6879 - val_precision_m: 0.6730 - val_recall_m: 0.7214 - val_auc: 0.9502\n",
            "Epoch 42/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1266 - acc: 0.9489 - f1_m: 0.7300 - precision_m: 0.7849 - recall_m: 0.6990 - auc: 0.9682 - val_loss: 0.1691 - val_acc: 0.9344 - val_f1_m: 0.6929 - val_precision_m: 0.6549 - val_recall_m: 0.7541 - val_auc: 0.9502\n",
            "Epoch 43/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1255 - acc: 0.9487 - f1_m: 0.7307 - precision_m: 0.7808 - recall_m: 0.7028 - auc: 0.9687 - val_loss: 0.1828 - val_acc: 0.9278 - val_f1_m: 0.6833 - val_precision_m: 0.6135 - val_recall_m: 0.7905 - val_auc: 0.9503\n",
            "Epoch 44/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1251 - acc: 0.9498 - f1_m: 0.7359 - precision_m: 0.7905 - recall_m: 0.7062 - auc: 0.9691 - val_loss: 0.1619 - val_acc: 0.9384 - val_f1_m: 0.6607 - val_precision_m: 0.7445 - val_recall_m: 0.6102 - val_auc: 0.9462\n",
            "Epoch 45/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1233 - acc: 0.9505 - f1_m: 0.7397 - precision_m: 0.7949 - recall_m: 0.7079 - auc: 0.9695 - val_loss: 0.1625 - val_acc: 0.9359 - val_f1_m: 0.6859 - val_precision_m: 0.6768 - val_recall_m: 0.7126 - val_auc: 0.9487\n",
            "Epoch 46/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1228 - acc: 0.9505 - f1_m: 0.7394 - precision_m: 0.7915 - recall_m: 0.7103 - auc: 0.9700 - val_loss: 0.1745 - val_acc: 0.9320 - val_f1_m: 0.6874 - val_precision_m: 0.6409 - val_recall_m: 0.7596 - val_auc: 0.9492\n",
            "Epoch 47/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1220 - acc: 0.9511 - f1_m: 0.7435 - precision_m: 0.7952 - recall_m: 0.7143 - auc: 0.9707 - val_loss: 0.1719 - val_acc: 0.9340 - val_f1_m: 0.6908 - val_precision_m: 0.6549 - val_recall_m: 0.7490 - val_auc: 0.9496\n",
            "Epoch 48/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1213 - acc: 0.9516 - f1_m: 0.7452 - precision_m: 0.7984 - recall_m: 0.7155 - auc: 0.9709 - val_loss: 0.1592 - val_acc: 0.9377 - val_f1_m: 0.6686 - val_precision_m: 0.7200 - val_recall_m: 0.6400 - val_auc: 0.9475\n",
            "Epoch 49/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1199 - acc: 0.9520 - f1_m: 0.7496 - precision_m: 0.7997 - recall_m: 0.7223 - auc: 0.9715 - val_loss: 0.1674 - val_acc: 0.9364 - val_f1_m: 0.6816 - val_precision_m: 0.6856 - val_recall_m: 0.6936 - val_auc: 0.9464\n",
            "Epoch 50/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1179 - acc: 0.9530 - f1_m: 0.7523 - precision_m: 0.8001 - recall_m: 0.7256 - auc: 0.9724 - val_loss: 0.1606 - val_acc: 0.9382 - val_f1_m: 0.6690 - val_precision_m: 0.7225 - val_recall_m: 0.6389 - val_auc: 0.9475\n",
            "Epoch 51/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1178 - acc: 0.9525 - f1_m: 0.7511 - precision_m: 0.7984 - recall_m: 0.7246 - auc: 0.9726 - val_loss: 0.1613 - val_acc: 0.9369 - val_f1_m: 0.6768 - val_precision_m: 0.6984 - val_recall_m: 0.6729 - val_auc: 0.9481\n",
            "Epoch 52/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1164 - acc: 0.9532 - f1_m: 0.7552 - precision_m: 0.8047 - recall_m: 0.7275 - auc: 0.9733 - val_loss: 0.1776 - val_acc: 0.9315 - val_f1_m: 0.6858 - val_precision_m: 0.6393 - val_recall_m: 0.7587 - val_auc: 0.9488\n",
            "Epoch 53/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1161 - acc: 0.9533 - f1_m: 0.7558 - precision_m: 0.8028 - recall_m: 0.7299 - auc: 0.9736 - val_loss: 0.1678 - val_acc: 0.9338 - val_f1_m: 0.6813 - val_precision_m: 0.6611 - val_recall_m: 0.7206 - val_auc: 0.9488\n",
            "Epoch 54/100\n",
            "1411/1411 [==============================] - 23s 16ms/step - loss: 0.1157 - acc: 0.9536 - f1_m: 0.7579 - precision_m: 0.8061 - recall_m: 0.7319 - auc: 0.9734 - val_loss: 0.1620 - val_acc: 0.9372 - val_f1_m: 0.6687 - val_precision_m: 0.7125 - val_recall_m: 0.6458 - val_auc: 0.9459\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f51d5c335d0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3 Testing results"
      ],
      "metadata": {
        "id": "wOYJ-kezVe-r"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Xi7UAdkRvTV",
        "outputId": "243a318e-79f0-45c7-ebef-172295d7809a"
      },
      "source": [
        "# model results\n",
        "train_pred2 = model2.predict(x_train)\n",
        "test_pred2 = model2.predict(x_val)\n",
        "\n",
        "train_pred_binary2 = np.where(train_pred2>= 0.48,1,0)\n",
        "test_pred_binary2 = np.where(test_pred2>= 0.48,1,0)\n",
        "# reshaping array \n",
        "train_pred_binary2 = train_pred_binary2.reshape(180536,)\n",
        "test_pred_binary2 = test_pred_binary2.reshape(120358,)\n",
        "\n",
        "# Accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "acc = sum(train_pred_binary2 == y_train)/  len(y_train)\n",
        "test_acc = sum(test_pred_binary2 == y_valid)/  len(y_valid)\n",
        "print('Accuracy of train model is {}'.format(acc))\n",
        "print('Accuracy of test model is {}'.format(test_acc))\n",
        "\n",
        "\n",
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_valid,test_pred_binary2))\n",
        "\n",
        "\n",
        "#  Recall  , f1 , precision \n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_valid,test_pred_binary2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of train model is 0.9457227367394868\n",
            "Accuracy of test model is 0.9275993286694694\n",
            "[[101831   6255]\n",
            " [  2459   9813]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96    108086\n",
            "           1       0.61      0.80      0.69     12272\n",
            "\n",
            "    accuracy                           0.93    120358\n",
            "   macro avg       0.79      0.87      0.83    120358\n",
            "weighted avg       0.94      0.93      0.93    120358\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Sample Text to test model 3 results."
      ],
      "metadata": {
        "id": "DYN3-8i5VoiC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b53pLWqfTIuF"
      },
      "source": [
        "ques = ['will democrats win 2021', \n",
        "        'is he still alive', \n",
        "        'name famous black celebreties',\n",
        "        'are republican not racists', \n",
        "        'distance to moon'\n",
        "        'which race is the smartest', \n",
        "        'what are achivements by trump' ,\n",
        "        'are republican racists',\n",
        "        'is india safe', \n",
        "        'are jews loyal', \n",
        "        'which race is not loyal',\n",
        "        'why republican cry always', \n",
        "        'does trump hate gay', \n",
        "        'do we require another pandamic',\n",
        "        'is america under dictatorship', \n",
        "        'are muslim terrorist', \n",
        "        'do you support taliban',\n",
        "        'are white people', \n",
        "        'are you fool', \n",
        "        'are muslim extremist', \n",
        "        'is china facist country' ,\n",
        "        'are democrats dumb', \n",
        "        'do blacks support gay',  \n",
        "        'do americans support holocaust']\n",
        "ques = preprocess_parallel(ques)\n",
        "\n",
        "' '.join(ques[0])\n",
        "# ques = ' '.join(preprocess_parallel(ques))\n",
        "\n",
        "join_corpus =[]\n",
        "for l in ques:\n",
        "  join_corpus.append(' '.join(l))\n",
        "\n",
        "\n",
        "testing = vectorizer(np.array([[s] for s in join_corpus])).numpy()\n",
        "\n",
        "pd.DataFrame({\n",
        "     'Inputs': ques\n",
        "    ,'results': model2.predict(testing)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving Model 3"
      ],
      "metadata": {
        "id": "Ci_EUYTmXP_i"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "7XK8ATxqQrih",
        "outputId": "b43e913b-a76e-4517-a960-e32cbebfee17"
      },
      "source": [
        "### Saving Distilbert Model\n",
        "\n",
        "#  Saving Model weights\n",
        "model2.save_weights('BiLSTM_CNN/my_model')\n",
        "\n",
        "### Loading saved model.\n",
        "# loading the model params on which it was trained\n",
        "load_model2 = BiLSTM_CNN()\n",
        "\n",
        "#  loading the training weights back to model.\n",
        "load_model2.load_weights('BiLSTM_CNN/my_model')\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "!zip -r /content/BiLSTM_CNN_v5.zip /content/BiLSTM_CNN\n",
        "files.download('BiLSTM_CNN_v5.zip')\n",
        "\n",
        "# model results\n",
        "train_pred2 = load_model2.predict(x_train)\n",
        "test_pred2 = load_model2.predict(x_val)\n",
        "\n",
        "\n",
        "print(train_pred2.shape, test_pred2.shape)\n",
        "\n",
        "train_pred_binary2 = np.where(train_pred2>= 0.48,1,0)\n",
        "test_pred_binary2 = np.where(test_pred2>= 0.48,1,0)\n",
        "# reshaping array \n",
        "train_pred_binary2 = train_pred_binary2.reshape(180536,)\n",
        "test_pred_binary2 = test_pred_binary2.reshape(120358,)\n",
        "\n",
        "# Accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "acc = sum(train_pred_binary2 == y_train)/  len(y_train)\n",
        "test_acc = sum(test_pred_binary2 == y_valid)/  len(y_valid)\n",
        "print('Accuracy of train model is {}'.format(acc))\n",
        "print('Accuracy of test model is {}'.format(test_acc))\n",
        "\n",
        "\n",
        "# Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_valid,test_pred_binary2))\n",
        "\n",
        "\n",
        "#  Recall  , f1 , precision \n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_valid,test_pred_binary2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/BiLSTM_CNN/ (stored 0%)\n",
            "  adding: content/BiLSTM_CNN/my_model.index (deflated 70%)\n",
            "  adding: content/BiLSTM_CNN/checkpoint (deflated 40%)\n",
            "  adding: content/BiLSTM_CNN/my_model.data-00000-of-00001 (deflated 16%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_c41baae4-cce9-4303-bc72-ed07d9fe579f\", \"BiLSTM_CNN_v5.zip\", 8074287)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(180536, 1) (120358, 1)\n",
            "Accuracy of train model is 0.9457227367394868\n",
            "Accuracy of test model is 0.9275993286694694\n",
            "[[101831   6255]\n",
            " [  2459   9813]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96    108086\n",
            "           1       0.61      0.80      0.69     12272\n",
            "\n",
            "    accuracy                           0.93    120358\n",
            "   macro avg       0.79      0.87      0.83    120358\n",
            "weighted avg       0.94      0.93      0.93    120358\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYTSmYcYWPdg"
      },
      "source": [
        "### Loading final Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZM6NY3SWgkJ"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/Final Model/BiLSTM_CNN_v3_final.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO1Krz9sW2q3",
        "outputId": "f05d0743-0a8c-4607-f5ef-20c75f727405"
      },
      "source": [
        "# loading the model params on which it was trained\n",
        "inference_model = BiLSTM_CNN()\n",
        "\n",
        "#  loading the training weights back to model.\n",
        "inference_model.load_weights('BiLSTM_CNN/my_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa6244ef890>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHleKhamXMxS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d4c4f63-9a4d-4ecc-83e9-f267064249f2"
      },
      "source": [
        "ques = ['will democrats win 2021', \n",
        "        'is he still alive', \n",
        "        'name famous black celebreties',\n",
        "        'are republican not racists', \n",
        "        'distance to moon'\n",
        "        'which race is the smartest', \n",
        "        'what are achivements by trump' ,\n",
        "        'are republican racists',\n",
        "        'is india safe', \n",
        "        'are jews loyal', \n",
        "        'which race is not loyal',\n",
        "        'why republican cry always', \n",
        "        'does trump hate gay', \n",
        "        'do we require another pandamic',\n",
        "        'is america under dictatorship', \n",
        "        'are muslim terrorist', \n",
        "        'do you support taliban',\n",
        "        'are white people', \n",
        "        'are you fool', \n",
        "        'are muslim extremist', \n",
        "        'is china facist country' ,\n",
        "        'are democrats dumb', \n",
        "        'do blacks support gay',  \n",
        "        'do americans support holocaust']\n",
        "ques = preprocess_parallel(ques)\n",
        "\n",
        "' '.join(ques[0])\n",
        "# ques = ' '.join(preprocess_parallel(ques))\n",
        "\n",
        "join_corpus =[]\n",
        "for l in ques:\n",
        "  join_corpus.append(' '.join(l))\n",
        "\n",
        "\n",
        "testing = vectorizer(np.array([[s] for s in join_corpus])).numpy()\n",
        "\n",
        "(inference_model.predict(testing))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.19312404],\n",
              "       [0.19258153],\n",
              "       [0.18131801],\n",
              "       [0.9749401 ],\n",
              "       [0.3616696 ],\n",
              "       [0.30859843],\n",
              "       [0.9526248 ],\n",
              "       [0.48837128],\n",
              "       [0.80390453],\n",
              "       [0.42273363],\n",
              "       [0.92296475],\n",
              "       [0.9560496 ],\n",
              "       [0.01709131],\n",
              "       [0.50699085],\n",
              "       [0.97760195],\n",
              "       [0.4158798 ],\n",
              "       [0.93195975],\n",
              "       [0.6864999 ],\n",
              "       [0.8287997 ],\n",
              "       [0.27979502],\n",
              "       [0.95841295],\n",
              "       [0.96839327],\n",
              "       [0.91232324]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### End"
      ],
      "metadata": {
        "id": "Nlt9U9IJXi4t"
      }
    }
  ]
}